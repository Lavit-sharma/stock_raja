name: Scraper 3 (Batch 1001â€“1500)
on:
  schedule:
    - cron: '20 1 * * *'
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9]
      max-parallel: 10

    env:
      SHARD_INDEX: ${{ matrix.shard }}
      SHARD_STEP: 10
      BATCH_START: 1000
      BATCH_END: 1500

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          pip install selenium beautifulsoup4 gspread webdriver-manager
      - run: |
          echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json
          echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json
      - run: |
          python run_scraper.py
        env:
          CHECKPOINT_FILE: checkpoint_batch3_${{ matrix.shard }}.txt
