name: Parallel TradingView Scrape (10x stagger)

on:
  workflow_dispatch:
  schedule:
    - cron: '30 3 * * *'  # daily

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        batch: [1,2,3,4,5,6,7,8,9,10]
      max-parallel: 10
      fail-fast: false

    steps:
      - uses: actions/checkout@v4

      - name: Install chrome (chromium)
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser
          chromium-browser --version || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install selenium webdriver-manager beautifulsoup4 gspread google-api-python-client

      - name: Compute start index
        id: calc
        run: |
          START=$(( ( ${{ matrix.batch }} - 1 ) * 200 + 1 ))
          echo "start=${START}" >> "$GITHUB_OUTPUT"
          echo "size=200" >> "$GITHUB_OUTPUT"

      - name: Stagger start (2 minutes per batch)
        run: sleep $(( ( ${{ matrix.batch }} - 1 ) * 120 ))

      - name: Run scraper
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
          COOKIES_JSON: ${{ secrets.COOKIES_JSON }}
          START_INDEX: ${{ steps.calc.outputs.start }}
          BATCH_SIZE: ${{ steps.calc.outputs.size }}
        run: |
          # prepare file with URLs (your repo should contain a urls list or you can modify to read from Google Sheets)
          # Example: read urls.txt sliced by START_INDEX and BATCH_SIZE (assuming file exists)
          START=${START_INDEX}
          SIZE=${BATCH_SIZE}
          # create slice file
          awk "NR>=(START) && NR<=(START+SIZE-1)" urls.txt > slice_urls.txt || true
          python run_tradingview_scraper.py --file slice_urls.txt --to-sheets
