name: Parallel Stock Scraper (Logged-In, 10x Parallel)

on:
  schedule:
    - cron: '30 3 * * *'  # Daily at 03:30 UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    strategy:
      # This matrix defines 10 parallel jobs
      matrix:
        batch: [1,2,3,4,5,6,7,8,9,10]
      max-parallel: 10
      fail-fast: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Show commit SHA
        run: echo "Running commit ${{ github.sha }}"

      # --- REMOVED: Install Chrome step ---
      # WebDriverManager now handles the driver, and Chromium is often pre-installed.
      # If you face issues, you can uncomment this block.

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          sudo apt-get update && sudo apt-get install -y chromium-browser # Ensure Chromium is ready
          python -m pip install --upgrade pip
          pip install gspread selenium beautifulsoup4 google-api-core packaging webdriver-manager

      - name: Calculate start index
        id: calc
        shell: bash
        env:
          MATRIX_BATCH: ${{ matrix.batch }}
        run: |
          # The list is 0-indexed in Python, but gspread.col_values(5) returns a 1-indexed list 
          # The logic is correct: Batch 1 starts at index 1 (the header is skipped in Python list slicing)
          START=$(( ( ${MATRIX_BATCH} - 1 ) * 200 + 1 ))  # 1, 201, 401, ...
          echo "start=${START}" >> "$GITHUB_OUTPUT"
          echo "size=200" >> "$GITHUB_OUTPUT"

      - name: Stagger start
        shell: bash
        env:
          MATRIX_BATCH: ${{ matrix.batch }}
        run: |
          # Staggering is crucial to prevent all 10 jobs from hitting Google Sheets/TradingView simultaneously.
          SLEEP=$(( ( ${MATRIX_BATCH} - 1 ) * 15 ))  # Increased stagger to 15s (0s, 15s, 30s,...)
          echo "Staggering ${SLEEP}s"
          sleep "${SLEEP}"

      - name: Run batch
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
          COOKIES_JSON: ${{ secrets.COOKIES_JSON }}
          START_INDEX: ${{ steps.calc.outputs.start }}
          BATCH_SIZE: ${{ steps.calc.outputs.size }}
        run: |
          echo "Batch ${{ matrix.batch }}: START_INDEX=$START_INDEX, SIZE=$BATCH_SIZE"
          python run_scraper.py # Assuming your python file is named run_scraper.py
