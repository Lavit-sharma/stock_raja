name: Daily Stock Scraper

on:
  schedule:
    # Runs daily at 9:00 AM IST (3:30 AM UTC)
    - cron: '30 3 * * *'
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of companies (default: 100)'
        required: false
        default: '100'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
          BATCH_SIZE: ${{ github.event.inputs.batch_size || '100' }}
          START_INDEX: '1'
        run: |
          python run_scraper.py
      
    - name: Upload logs (if failed)
  if: failure()
  uses: actions/upload-artifact@v4    # ‚Üê v3 se v4 kar diya
  with:
    name: error-logs
    path: |
      *.log
      *.html
