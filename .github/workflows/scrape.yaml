name: TradingView Scraper

on:
  schedule:
    # Runs at 1:00 AM UTC every day
    - cron: '0 1 * * *'
  # Allows manual triggering
  workflow_dispatch:

jobs:
  # =========================================================
  # JOB 1: READ DATA ONCE & SAVE AS ARTIFACT
  # This prevents 10 simultaneous 'Read requests' that cause the 429 error.
  # =========================================================
  prepare_data:
    name: 1. Prepare Stock List (Single Read)
    runs-on: ubuntu-latest
    outputs:
      success: ${{ steps.prepare.outcome }}

    steps:
      - name: â¬‡ï¸ Checkout Repository
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: ðŸ“¦ Install dependencies
        run: pip install gspread

      - name: ðŸ”‘ Set up Google Credentials
        run: echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json

      - name: âš™ï¸ Read and Save Data Locally
        id: prepare
        # This script must exist and must contain the code to read the Sheet and save to company_list.json and name_list.json
        run: python prepare_data.py
        
      - name: â¬†ï¸ Upload Data Artifact
        uses: actions/upload-artifact@v3
        with:
          name: stock-data
          # Upload the generated local JSON files
          path: |
            company_list.json
            name_list.json
          
  ---
  
  # =========================================================
  # JOB 2: PARALLEL SCRAPING (10 Jobs)
  # =========================================================
  scrape:
    needs: prepare_data # Wait for data preparation
    
    strategy:
      fail-fast: false
      matrix:
        # Ten chunks (0 to 9)
        chunk: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 
        chunk_size: [250] # 2500 stocks / 10 jobs = 250 per job
        start_index: [1]

    # Corrected syntax: ${{ expression }} is required for arithmetic in job names
    name: Scrape Chunk ${{ matrix.chunk + 1 }}/10

    runs-on: ubuntu-latest
    
    steps:
      - name: â¬‡ï¸ Checkout Repository
        uses: actions/checkout@v4

      - name: â¬‡ï¸ Download Data Artifact
        uses: actions/download-artifact@v3
        with:
          name: stock-data
          path: . # Files (company_list.json, name_list.json) are downloaded here
          
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: ðŸ“¦ Install dependencies
        run: pip install selenium beautifulsoup4 gspread webdriver-manager

      - name: ðŸ”‘ Set up Google Credentials
        run: echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json

      - name: ðŸª Set up TradingView Cookies
        run: echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json
      
      - name: âš™ï¸ Set Chunk Indices
        id: indices
        run: |
          # ðŸ’¡ FIX: Using shell arithmetic $(( )) nested with ${{ }} for calculation.
          # Calculates 1-based start index (e.g., 501)
          START=$(( ${{ matrix.start_index }} + (${{ matrix.chunk }} * ${{ matrix.chunk_size }}) ))
          # Calculates 1-based end index (e.g., 750)
          END=$(( $START + ${{ matrix.chunk_size }} - 1 ))
          echo "Current Job will scrape from index $START to $END"
          
          # Set the environment variables
          echo "CHUNK_START_INDEX=$START" >> $GITHUB_ENV
          echo "CHUNK_END_INDEX=$END" >> $GITHUB_ENV

      # Creates the checkpoint file with the job's starting index
      - name: âš™ï¸ Create Checkpoint File
        run: echo "${{ env.CHUNK_START_INDEX }}" > checkpoint_new_1.txt

      - name: ðŸš€ Run Scraper Script
        # This script (run_scraper.py) reads the local JSON files and the env vars
        run: python run_scraper.py
